{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar sentences generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This notebook tries to generate similar sentences to a given input sentence. \n",
    "- It makes use of WordNet and GloVe embeddings to arrive at substitute words for candidate words in a sentence.\n",
    "- Similarity threshold, and number of sentences generated can be controlled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Glove Vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to glove vectors\n",
    "glove_path = \"../Word2vec/gensim_glove.6B.50d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Word2vec/gensim_glove.6B.50d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\KAURNA~1\\AppData\\Local\\Temp/ipykernel_17648/4272022775.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load GloVe vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeyedvectors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mglove_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1117\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1118\u001b[0m             \u001b[0mWord2VecKeyedVectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1119\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    496\u001b[0m     \u001b[0mignore_ext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m     )\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'errors'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 361\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Word2vec/gensim_glove.6B.50d.txt'"
     ]
    }
   ],
   "source": [
    "# load GloVe vectors\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(glove_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the vocab\n",
    "vocab = glove_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the GloVe vectors\n",
    "len(vocab.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK + WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk is used to perform POS tagging\n",
    "import nltk \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility method to return \"simple\" POS tag for a given POS tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pos_identity(pos_tag):\n",
    "    \n",
    "    '''\n",
    "    This method returns\n",
    "    \n",
    "    1. 'np' for proper nouns, 'n' for all other nouns\n",
    "    \n",
    "    2. 'a' for adjectives\n",
    "    \n",
    "    3. 'v' for verbs\n",
    "    \n",
    "    4. 'r' for adverbs\n",
    "    \n",
    "    5. None for all other tags\n",
    "    '''\n",
    "    \n",
    "    if pos_tag in ['NNP', 'NNPS']:\n",
    "        return 'np'\n",
    "    elif pos_tag in ['NN', 'NNS']:\n",
    "        return 'n'\n",
    "    elif pos_tag in ['JJ', 'JJR', 'JJS']:\n",
    "        return 'a'\n",
    "    elif pos_tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        return 'v'\n",
    "    elif pos_tag in ['RB', 'RBR', 'RBS']:\n",
    "        return 'r'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return most similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_words(word, pos_tag, similarity_threshold):\n",
    "    \n",
    "    '''\n",
    "    This method returns most similar words to the word passed.\n",
    "    \n",
    "    args:\n",
    "    \n",
    "    word = input word\n",
    "    pos_tag = Simple POS tag of the word\n",
    "    similarity_threshold (float) = Value between 0 and 1. Indicates the similarity threshold to consider\n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    a list of similar words, along with the original word\n",
    "    '''\n",
    "\n",
    "    # Lemmatize the word\n",
    "    word = lemmatizer.lemmatize(word, pos_tag)\n",
    "    # Get the synonyms and antonyms of a word\n",
    "    synonyms = [word] \n",
    "    #antonyms = [] \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        vector_check = glove_model.wv.get_vector(word)\n",
    "    except:\n",
    "        # If the word does not exist in the Glove model, return\n",
    "        return synonyms\n",
    "\n",
    "    for syn in wordnet.synsets(word): \n",
    "    \n",
    "        for l in syn.lemmas():\n",
    "        \n",
    "            try:\n",
    "            \n",
    "                if l.name() in synonyms:\n",
    "                    continue\n",
    "                \n",
    "                # Get the vector of the synonym\n",
    "                vector_prospect = glove_model.wv.get_vector(l.name())\n",
    "            \n",
    "                #print('Checking word = ', l.name())\n",
    "                cosine_diff = vocab.cosine_similarities(vector_1=vector_check, vectors_all=[vector_prospect])\n",
    "                #print(cosine_diff)\n",
    "            \n",
    "                #similar_by_vector()words_closer_than()n_similarity()\n",
    "                if cosine_diff > similarity_threshold:\n",
    "                    synonyms.append(l.name()) \n",
    "            \n",
    "            except:\n",
    "                \n",
    "                pass\n",
    "        \n",
    "            #if l.antonyms(): \n",
    "             #   antonyms.append(l.antonyms()[0].name()) \n",
    "            \n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_position(total_synonym_array, position_array, last_position):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    This method returns the next position of word replacement.\n",
    "    \n",
    "    args:\n",
    "    \n",
    "    total_synonym_array = Array containing the total length of synonyms\n",
    "    position_array = Array containing current positions\n",
    "    last_position_array = Integer\n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    next position to be updated, -1 if all positions are exhausted\n",
    "    '''\n",
    "    new_pos = last_position\n",
    "    \n",
    "    for i in range(len(total_synonym_array)):\n",
    "        \n",
    "        # get a new position\n",
    "        new_pos = (new_pos + 1) % len(total_synonym_array)\n",
    "        \n",
    "        # if the new position is not good enough, fetch a new one\n",
    "        if position_array[new_pos] == -1 or position_array[new_pos] == total_synonym_array[new_pos]:\n",
    "            continue\n",
    "        else:\n",
    "            return new_pos\n",
    "\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position_arrays(sentence_combination):\n",
    "    \n",
    "    '''\n",
    "    This is a utility method to get position arrays.\n",
    "    \n",
    "    args:\n",
    "    \n",
    "    sentence_combination = [[word], [word1, word2, ]]\n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    two position arrays\n",
    "    '''\n",
    "    total_synonym_array = []\n",
    "    initial_position_array = []\n",
    "    \n",
    "    for each_word_array in sentence_combination:\n",
    "        length = len(each_word_array)\n",
    "        total_synonym_array.append(length)\n",
    "        if length == 1:\n",
    "            initial_position_array.append(-1)\n",
    "        else:\n",
    "            initial_position_array.append(0)\n",
    "    \n",
    "    return total_synonym_array, initial_position_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to provide an alternate sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_alternate_sentence(sentence, num_versions=1, max_changes=1, similarity_threshold=0.7, ignore_stopwords=True, ignore_proper_nouns=True):\n",
    "    \n",
    "    '''\n",
    "    This method returns an alternate version(s) of the sentence passed by replacing words with their closest synonyms.\n",
    "    \n",
    "    args:\n",
    "    \n",
    "    sentence (String) = the input sentence\n",
    "    num_versions (int) = the number of alternate versions required\n",
    "    max_changes (int) = the maximum number of changes between versions\n",
    "    similarity_threshold (float) = Value between 0 and 1. Indicates the similarity threshold to consider while replacing words\n",
    "    ignore_stopwords (bool) = If True, stopwords will not be considered for replacement\n",
    "    ignore_proper_nouns (bool) = If True, proper nouns will be ignored for replacement\n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    list of alternate sentence(s)\n",
    "    '''\n",
    "    \n",
    "    alternate_sentences = []\n",
    "    \n",
    "    sentence_combination = []\n",
    "    \n",
    "    # split the sentence into words\n",
    "    words = sentence.split()\n",
    "    \n",
    "    # pos tag the sentence\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    \n",
    "    \n",
    "    for each_word_pos in pos_tags:\n",
    "        \n",
    "        word = each_word_pos[0]\n",
    "        pos_tag = each_word_pos[1]\n",
    "        short_pos = fetch_pos_identity(pos_tag)\n",
    "        \n",
    "        # ignore proper nouns\n",
    "        if ignore_proper_nouns and 'np' == short_pos:\n",
    "            sentence_combination.append([word])\n",
    "            continue\n",
    "        \n",
    "        # lemmatize the word\n",
    "        if short_pos is not None:\n",
    "            word_lemmatized = lemmatizer.lemmatize(word, short_pos)\n",
    "        else:\n",
    "            word_lemmatized = lemmatizer.lemmatize(word)\n",
    "        \n",
    "        # ignore stopwords\n",
    "        if ignore_stopwords and (word_lemmatized in stop_words or word in stop_words):\n",
    "            sentence_combination.append([word])\n",
    "            continue\n",
    "        \n",
    "        # if POS is noun, adj, adv, or verb - get similar words\n",
    "        if short_pos is not None:\n",
    "            sentence_combination.append(get_related_words(word, short_pos, similarity_threshold))\n",
    "        # else do nothing\n",
    "        else:\n",
    "            sentence_combination.append([word])\n",
    "            continue\n",
    "    \n",
    "    total_synonym_array, position_array = get_position_arrays(sentence_combination)\n",
    "    \n",
    "    total_combos_possible = 0\n",
    "    for some_value in total_synonym_array:\n",
    "        if some_value > 1:\n",
    "            total_combos_possible = total_combos_possible + some_value\n",
    "    \n",
    "    total_combos_possible = total_combos_possible - 1\n",
    "    \n",
    "    last_position = -1\n",
    "    \n",
    " \n",
    "    for i in range(num_versions):\n",
    "        \n",
    "        if i >= total_combos_possible:\n",
    "            break\n",
    "        \n",
    "        # get the position to replace\n",
    "        position = get_next_position(total_synonym_array, position_array, last_position)\n",
    "        \n",
    "        #print(position)\n",
    "        \n",
    "        if position == -1:\n",
    "            break\n",
    "        \n",
    "        alt_sentence = ''\n",
    "        counter = 0\n",
    "        for j in sentence_combination:\n",
    "            alt_sentence = alt_sentence + ' '\n",
    "            if counter == position:\n",
    "                alt_sentence = alt_sentence + j[position_array[position]] # ] + 1 ]\n",
    "                position_array[position] = position_array[position] + 1\n",
    "                \n",
    "                last_position = position\n",
    "                \n",
    "            else:\n",
    "                if position_array[counter] > -1:\n",
    "                    alt_sentence = alt_sentence + j[position_array[counter] - 1]\n",
    "                else:\n",
    "                    alt_sentence = alt_sentence + j[position_array[counter]]\n",
    "                \n",
    "            \n",
    "            counter = counter + 1\n",
    "        \n",
    "        alt_sentence = alt_sentence.strip()\n",
    "        alternate_sentences.append(alt_sentence)\n",
    "            \n",
    "    return alternate_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provide_alternate_sentence('We collect your information regularly', num_versions=4, similarity_threshold=0.60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
