{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "103d27af",
   "metadata": {},
   "source": [
    "Wordnet Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43b68ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.5.0 in c:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk==3.5.0) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk==3.5.0) (4.63.0)\n",
      "Requirement already satisfied: click in c:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk==3.5.0) (8.0.4)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: regex in c:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk==3.5.0) (2022.3.2)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from click->nltk==3.5.0) (4.8.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from click->nltk==3.5.0) (0.4.5)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata->click->nltk==3.5.0) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata->click->nltk==3.5.0) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\kaurnavdeep1\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk==3.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84d21e3",
   "metadata": {},
   "source": [
    "Import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7abe4c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kaurnavdeep1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bedffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distractors from Wordnet\n",
    "def get_distractors_wordnet(syn,word):\n",
    "    distractors=[]\n",
    "    word= word.lower()\n",
    "    orig_word = word\n",
    "    if len(word.split())>0:\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    hypernym = syn.hypernyms()\n",
    "    if len(hypernym) == 0: \n",
    "        return distractors\n",
    "    for item in hypernym[0].hyponyms():\n",
    "        name = item.lemmas()[0].name()\n",
    "        #print (\"name \",name, \" word\",orig_word)\n",
    "        if name == orig_word:\n",
    "            continue\n",
    "        name = name.replace(\"_\",\" \")\n",
    "        name = \" \".join(w.capitalize() for w in name.split())\n",
    "        if name is not None and name not in distractors:\n",
    "            distractors.append(name)\n",
    "    return distractors\n",
    "original_word = \"Festival\"\n",
    "synset_to_use = wn.synsets(original_word,'n')[0]\n",
    "distractors_calculated = get_distractors_wordnet(synset_to_use,original_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f167d479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Festival\n"
     ]
    }
   ],
   "source": [
    "print(original_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62103c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Air Alert', 'Bimester', 'Bimillennium', 'Bout', 'Bronze Age', 'Calendar Day', 'Calendar Month', 'Century', 'Clotting Time', 'Dawn', 'Day', 'Decade', 'Dog Days', 'Downtime', 'Drought', 'Duration', 'Elapsed Time', 'Enlistment', 'Era', 'Eve', 'Evening', 'Field Day', 'Flower', 'Fortnight', 'Generation', 'Golden Age', 'Great Year', 'Half-century', 'Half Life', 'Honeymoon', 'Hospitalization', 'Hour', 'Hours', 'Incubation Period', 'Indian Summer', 'Indiction', 'Iron Age', 'Lactation', 'Lease', 'Life', 'Long Run', 'Long Time', 'Lustrum', 'Mid-april', 'Mid-august', 'Mid-december', 'Mid-february', 'Mid-january', 'Mid-july', 'Mid-june', 'Mid-march', 'Mid-may', 'Mid-november', 'Mid-october', 'Mid-september', 'Midweek', 'Midwinter', 'Millennium', 'Morning', 'Multistage', 'Night', 'Novitiate', 'Occupation', 'Olympiad', 'Overtime', 'Past', 'Peacetime', 'Phase', 'Phase Of The Moon', 'Prehistory', 'Prohibition', 'Puerperium', 'Quadrennium', 'Quarter-century', 'Quarter', 'Question Time', 'Quinquennium', 'Rainy Day', 'Real Time', 'Regulation Time', 'Reign', 'Run', 'Running Time', 'School', 'Season', 'Semester', 'Shelf Life', 'Silly Season', 'Silver Age', 'Sleep', 'Study Hall', 'Term', 'Tide', 'Time', 'Time Frame', 'Time Limit', 'Time Of Life', 'Time Off', 'Times', 'Travel Time', 'Trial Period', 'Trimester', 'Uptime', 'Usance', 'Wartime', 'Watch', 'Week', 'Weekend', 'Window', 'Work Time', 'Year', 'Youth']\n"
     ]
    }
   ],
   "source": [
    "print(distractors_calculated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c4ac5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_word: Festival\n",
      "Distractors_calculated: ['Air Alert', 'Bimester', 'Bimillennium', 'Bout', 'Bronze Age', 'Calendar Day', 'Calendar Month', 'Century', 'Clotting Time', 'Dawn', 'Day', 'Decade', 'Dog Days', 'Downtime', 'Drought', 'Duration', 'Elapsed Time', 'Enlistment', 'Era', 'Eve', 'Evening', 'Field Day', 'Flower', 'Fortnight', 'Generation', 'Golden Age', 'Great Year', 'Half-century', 'Half Life', 'Honeymoon', 'Hospitalization', 'Hour', 'Hours', 'Incubation Period', 'Indian Summer', 'Indiction', 'Iron Age', 'Lactation', 'Lease', 'Life', 'Long Run', 'Long Time', 'Lustrum', 'Mid-april', 'Mid-august', 'Mid-december', 'Mid-february', 'Mid-january', 'Mid-july', 'Mid-june', 'Mid-march', 'Mid-may', 'Mid-november', 'Mid-october', 'Mid-september', 'Midweek', 'Midwinter', 'Millennium', 'Morning', 'Multistage', 'Night', 'Novitiate', 'Occupation', 'Olympiad', 'Overtime', 'Past', 'Peacetime', 'Phase', 'Phase Of The Moon', 'Prehistory', 'Prohibition', 'Puerperium', 'Quadrennium', 'Quarter-century', 'Quarter', 'Question Time', 'Quinquennium', 'Rainy Day', 'Real Time', 'Regulation Time', 'Reign', 'Run', 'Running Time', 'School', 'Season', 'Semester', 'Shelf Life', 'Silly Season', 'Silver Age', 'Sleep', 'Study Hall', 'Term', 'Tide', 'Time', 'Time Frame', 'Time Limit', 'Time Of Life', 'Time Off', 'Times', 'Travel Time', 'Trial Period', 'Trimester', 'Uptime', 'Usance', 'Wartime', 'Watch', 'Week', 'Weekend', 'Window', 'Work Time', 'Year', 'Youth']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original_word:\",original_word)\n",
    "print(\"Distractors_calculated:\",distractors_calculated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f14e5aa",
   "metadata": {},
   "source": [
    " # two senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24b0b000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('cricket.n.01') :  leaping insect; male makes chirping noises by rubbing the forewings together \n",
      "\n",
      "Synset('cricket.n.02') :  a game played with a ball and bat by two teams of 11 players; teams take turns trying to score runs \n",
      "\n",
      "\n",
      "original word:  Cricket\n",
      "['Grasshopper']\n",
      "\n",
      "original word:  Cricket\n",
      "['Ball Game', 'Field Hockey', 'Football', 'Hurling', 'Lacrosse', 'Polo', 'Pushball', 'Ultimate Frisbee']\n"
     ]
    }
   ],
   "source": [
    "#  An example of a word with two different senses\n",
    "original_word = \"cricket\"\n",
    "syns = wn.synsets(original_word,'n')\n",
    "for syn in syns:\n",
    "  print (syn, \": \",syn.definition(),\"\\n\" )\n",
    "synset_to_use = wn.synsets(original_word,'n')[0]\n",
    "distractors_calculated = get_distractors_wordnet(synset_to_use,original_word)\n",
    "print (\"\\noriginal word: \",original_word.capitalize())\n",
    "print (distractors_calculated)\n",
    "original_word = \"cricket\"\n",
    "synset_to_use = wn.synsets(original_word,'n')[1]\n",
    "distractors_calculated = get_distractors_wordnet(synset_to_use,original_word)\n",
    "print (\"\\noriginal word: \",original_word.capitalize())\n",
    "print (distractors_calculated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc7b77",
   "metadata": {},
   "source": [
    "# 2. ConceptNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f0a089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import pprint\n",
    "# Distractors from http://conceptnet.io/\n",
    "def get_distractors_conceptnet(word):\n",
    "    word = word.lower()\n",
    "    original_word= word\n",
    "    if (len(word.split())>0):\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    distractor_list = [] \n",
    "    url = \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5\"%(word,word)\n",
    "    obj = requests.get(url).json()\n",
    "\n",
    "    for edge in obj['edges']:\n",
    "        link = edge['end']['term'] \n",
    "\n",
    "        url2 = \"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\"%(link,link)\n",
    "        obj2 = requests.get(url2).json()\n",
    "        for edge in obj2['edges']:\n",
    "            word2 = edge['start']['label']\n",
    "            if word2 not in distractor_list and original_word.lower() not in word2.lower():\n",
    "                distractor_list.append(word2)\n",
    "                   \n",
    "    return distractor_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5778deec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word:  Feature\n",
      "\n",
      "Distractors  ['An eye', 'eye', 'eyebrow', 'nose', 'facial muscle', 'jaw', 'beard', 'jowl', 'facial', 'sports section', 'news item', 'rotogravure', 'news article', 'column', 'comic strip', 'headline', 'news', 'public announcments', 'center spread', 'magazine article']\n"
     ]
    }
   ],
   "source": [
    "original_word = \"Feature\"\n",
    "distractors = get_distractors_conceptnet(original_word)\n",
    "\n",
    "print (\"Original word: \",original_word)\n",
    "print (\"\\nDistractors \",distractors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4084131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://api.dictionaryapi.dev/api/v2/entries/en/morning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b44915fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distractors_conceptnet(word):\n",
    "    word = word.lower()\n",
    "    original_word= word\n",
    "    if (len(word.split())>0):\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    distractor_list = [] \n",
    "    url = \"https://api.dictionaryapi.dev/api/v2/entries/en/{word}\"(word,word)\n",
    "    obj = requests.get(url).json()\n",
    "    \n",
    "#     for edge in obj['edges']:\n",
    "#         link = edge['end']['term'] \n",
    "\n",
    "#         url2 = \"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\"%(link,link)\n",
    "#         obj2 = requests.get(url2).json()\n",
    "#         for edge in obj2['edges']:\n",
    "#             word2 = edge['start']['label']\n",
    "#             if word2 not in distractor_list and original_word.lower() not in word2.lower():\n",
    "#                 distractor_list.append(word2)\n",
    "                   \n",
    "    return distractor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ec5d4c39",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\KAURNA~1\\AppData\\Local\\Temp/ipykernel_15528/909640164.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0moriginal_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Feature\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdistractors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_distractors_conceptnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Original word: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moriginal_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nDistractors \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdistractors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\KAURNA~1\\AppData\\Local\\Temp/ipykernel_15528/3810806425.py\u001b[0m in \u001b[0;36mget_distractors_conceptnet\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdistractor_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://api.dictionaryapi.dev/api/v2/entries/en/{word}\"\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "original_word = \"Feature\"\n",
    "distractors = get_distractors_conceptnet(original_word)\n",
    "\n",
    "print (\"Original word: \",original_word)\n",
    "print (\"\\nDistractors \",distractors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0bb9fde",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'answer_embedd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\KAURNA~1\\AppData\\Local\\Temp/ipykernel_14484/255223175.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_doc_similarity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeywords_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[0mfinal_distractors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmmr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer_embedd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdistractor_embedds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdistractors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[0mfiltered_distractors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfinal_distractors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'answer_embedd' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def mmr(doc_embedding: np.ndarray,\n",
    "        word_embeddings: np.ndarray,\n",
    "        words: List[str],\n",
    "        top_n: int = 5,\n",
    "        diversity: float = 0.9) -> List[Tuple[str, float]]:\n",
    "    \"\"\" Calculate Maximal Marginal Relevance (MMR)\n",
    "    between candidate keywords and the document.\n",
    "    MMR considers the similarity of keywords/keyphrases with the\n",
    "    document, along with the similarity of already selected\n",
    "    keywords and keyphrases. This results in a selection of keywords\n",
    "    that maximize their within diversity with respect to the document.\n",
    "    Arguments:\n",
    "        doc_embedding: The document embeddings\n",
    "        word_embeddings: The embeddings of the selected candidate keywords/phrases\n",
    "        words: The selected candidate keywords/keyphrases\n",
    "        top_n: The number of keywords/keyhprases to return\n",
    "        diversity: How diverse the select keywords/keyphrases are.\n",
    "                   Values between 0 and 1 with 0 being not diverse at all\n",
    "                   and 1 being most diverse.\n",
    "    Returns:\n",
    "         List[Tuple[str, float]]: The selected keywords/keyphrases with their distances\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract similarity within words, and between words and the document\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphras\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [(words[idx], round(float(word_doc_similarity.reshape(1, -1)[0][idx]), 4)) for idx in keywords_idx]\n",
    "  \n",
    "final_distractors = mmr(answer_embedd,distractor_embedds,distractors,5)\n",
    "filtered_distractors = []\n",
    "for dist in final_distractors:\n",
    "  filtered_distractors.append (dist[0])\n",
    "  \n",
    "Answer = filtered_distractors[0]\n",
    "Filtered_Distractors =  filtered_distractors[1:]\n",
    "\n",
    "print (Answer)\n",
    "print (\"------------------->\")\n",
    "for k in Filtered_Distractors:\n",
    "  print (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c6bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
